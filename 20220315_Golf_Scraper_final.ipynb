{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0637ff",
   "metadata": {},
   "source": [
    "Import necessary databases: BeautifulSoup, urllib, time, json, datetime, re, os and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b18af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer  # HTML parsing\n",
    "import urllib.request  # aufrufen von URLs\n",
    "from time import sleep  # damit legen wir den Scraper schlafen\n",
    "import json  # lesen und schreiben von JSON-Dateien\n",
    "from datetime import datetime  # um den Daten Timestamps zu geben\n",
    "import re  # regular expressions\n",
    "import os  # Dateipfade erstellen und lesen\n",
    "import pandas as pd  # Datenanalyse und -manipulation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d64f882",
   "metadata": {},
   "source": [
    "Check if folders (one to store a json with all the already visited urls to avoid duplicates, one to sotre the actual scraped data) already exist. If not: create them! Same for the json that holds the already visited urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4368d6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/michaelacker/Desktop/data/visited/ existiert bereits\n",
      "/Users/michaelacker/Desktop/data/Golf/ existiert bereits\n"
     ]
    }
   ],
   "source": [
    "folders = [\".../data/visited/\", \".../data/Golf/\"]\n",
    "for folder in folders:\n",
    "    if not os.path.isdir(folder):\n",
    "        os.makdirs(folder)\n",
    "        print(path, \"erstellt.\")\n",
    "    else:\n",
    "        print(folder, \"existiert bereits\")\n",
    "    \n",
    "path_to_visited_urls = \".../data/visited/visited_urls.json\"\n",
    "if not os.path.isfile(path_to_visited_urls):\n",
    "    with open(path_to_visited_urls, \"w\") as file:\n",
    "        json.dump([], file)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403471ef",
   "metadata": {},
   "source": [
    "The variable countries holds the countries that the scraper is supposed to scrape through. It can be either a list of countries (commented) or a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff0e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#countries = {\"Deutschland\": \"D\",\n",
    "            # \"Oesterreich\": \"A\",\n",
    "             #\"Belgien\": \"B\",\n",
    "             #\"Spanien\": \"E\",\n",
    "             #\"Frankreich\": \"F\",\n",
    "             #\"Italien\": \"I\",\n",
    "             #\"Luxemburg\": \"L\",\n",
    "             #\"Niederlande\": \"NL\"}\n",
    "countries = {\"Deutschland\": \"D\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3edf23",
   "metadata": {},
   "source": [
    "In an endless loop, the scraper will visit the search-page on autoscout24 that holds all offerings for Volkswagen Golf, first registered from 2008, sorted descending by age. From the sourcecode of that page it will idetify all urls that lead to a offered car (the ones who contain \"/angebote/\") and checks for every url if it is already in the \"visited-urls\"-list. If no, it appends it to the car_URLs list. This process will be repeated for all of the 20 pages that autoscout24 offers on a simple search.\n",
    "\n",
    "If after the 20th page there are new (not yet visited cars) in the car_URLs list, it will scrape through all of them and extract a remarkable variety of features for every, appends those features to a dictionary (car_dict) and appends that dictionary finally to a overall dictionary (multiple_cars_dict) that holds all scraped cars.\n",
    "\n",
    "Last but not least, multiple_car_dict is transfered into a pandas dataframe, several less important columns are deleted and the dataframe is saved as a csv for further wrangling and analysis.\n",
    "\n",
    "The loop will start again untill stopped manually. If no new car offers are available, the loop will reast for 60seconds befor starting over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "015bf570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate counter variables\n",
    "car_counter = 1\n",
    "cycle_counter = 0\n",
    "\n",
    "# initiate endless loop\n",
    "while True:\n",
    "    with open(path_to_visited_urls) as file:\n",
    "        visited_urls = json.load(file)\n",
    "    \n",
    "    # If more than 100000 have been visited, visited_urls will be cleared\n",
    "    if len(visited_urls) > 100000:\n",
    "        visited_urls = []\n",
    "\n",
    "    multiple_cars_dict = {}\n",
    "\n",
    "    cycle_counter += 1\n",
    "    \n",
    "    # When more then 1 country has been assigned for scraping, this will iterate through all of them\n",
    "    for country in countries:\n",
    "        car_URLs = []\n",
    "        \n",
    "        # Loop to iterate through page 1-20 and extract all search results\n",
    "        for page in range(1, 21):\n",
    "            try:\n",
    "                url = \"https://www.autoscout24.de/lst/volkswagen/golf?fregfrom=2008&sort=age&\"\\\n",
    "                      f\"desc=1&page={str(page)}&cy=D&atype=C&ustate=N%2CU&fuel=D%2CB&powertype=kw&custtype=D\"\\\n",
    "                      \"&ocs_listing=include&search_id=2c1t48wb05m\"\n",
    "                only_a_tags = SoupStrainer(\"a\")\n",
    "                \n",
    "                # Create soup objekt for further scraping\n",
    "                soup = BeautifulSoup(urllib.request.urlopen(url).read(), \"html.parser\", parse_only=only_a_tags)\n",
    "            # Excepting handling if url is not valid anymore\n",
    "            except Exception as e:\n",
    "                print(\"Übersicht: \" + str(e) + \" \" * 50, end=\"\\r\")\n",
    "                pass\n",
    "            \n",
    "            for link in soup.find_all(\"a\"):\n",
    "                if r\"/angebote/\" in str(link.get(\"href\")):\n",
    "                    car_URLs.append(link.get(\"href\"))\n",
    "                    \n",
    "            car_URLs_unique = [car for car in list(set(car_URLs)) if car not in visited_urls]\n",
    "            print(f'Lauf {cycle_counter} | {country} | Seite {page} | {len(car_URLs_unique)} neue URLs', end=\"\\r\")\n",
    "        print(\"\")\n",
    "        \n",
    "       \n",
    "        if len(car_URLs_unique) > 0:\n",
    "            counter = 1\n",
    "            \n",
    "            # Loop to iterate through all search results and extract data\n",
    "            for URL in car_URLs_unique:\n",
    "                print(f'Lauf {cycle_counter} | {country} | Auto {car_counter}' + ' ' * 50, end=\"\\r\")\n",
    "                try:\n",
    "                    # country, date\n",
    "                    car_counter += 1\n",
    "                    car_dict = {}\n",
    "                    car_dict[\"country\"] = country\n",
    "                    car_dict[\"date\"] = str(datetime.now())\n",
    "                    car = BeautifulSoup(urllib.request.urlopen('https://www.autoscout24.de' + URL).read(), \"html5lib\")\n",
    "\n",
    "                    for key, value in zip(car.find_all(\"dt\"), car.find_all(\"dd\")):\n",
    "                        car_dict[key.text.replace(\"\\n\", \"\")] = value.text.replace(\"\\n\", \"\")\n",
    "\n",
    "                    # Marke, Modell\n",
    "                    car_dict[\"Marke\"] = car.find_all(\"a\", attrs={'class': 'css-5xfxbi'})[0].text\n",
    "                    car_dict[\"Modell\"] = car.find_all(\"a\", attrs={'class': 'css-5xfxbi'})[1].text.split(\" \")[-1]\n",
    "                    \n",
    "                    # Ort, price\n",
    "                    car_dict[\"ort\"] = car.find(\"a\", attrs={\"class\": \"css-u2icft\"}).text\n",
    "                    price = car.find(\"span\", attrs={\"class\": \"css-1tk7gp4\"}).text\n",
    "                    price = price[2:price.find(\",-\")]\n",
    "                    price = price.replace(\".\", \"\")\n",
    "                    car_dict[\"price\"] = price\n",
    "\n",
    "                    # Versch Merkmale\n",
    "                    merkmale = [\"Laufleistung\", \"Getriebe\", \"Erstzulassung\", \"Kraftstoff\", \"Leistung\", \"Haendler/Privat\"]\n",
    "\n",
    "                    i = 0\n",
    "                    for tag in car.find_all(\"div\", attrs={'class': 'css-16ceglb'}):\n",
    "                        tag = str(tag)\n",
    "                        tag = tag[tag.find(\">\") + 1:tag.rfind(\"<\")]\n",
    "                        car_dict[merkmale[i]] = tag\n",
    "                        i += 1\n",
    "\n",
    "\n",
    "\n",
    "                    # Extracting from a field of non-seperated, vary data\n",
    "                    merkmale = []\n",
    "                    werte = []\n",
    "                    \n",
    "                    i = 0\n",
    "                    for tag in car.find_all(\"dd\", attrs={'class': 'css-uvt9sy'}):\n",
    "                        tag = str(tag)\n",
    "                        tag = tag[tag.find(\">\") + 1:tag.rfind(\"<\")]\n",
    "                        werte.append(tag)\n",
    "\n",
    "                    for tag in car.find_all(\"span\", attrs={'class': 'css-1cmkyz9 e1n6hwap0'}):\n",
    "                        tag = str(tag)\n",
    "                        tag = tag[tag.find(\">\") + 1:tag.rfind(\"<\")]\n",
    "                        merkmale.append(tag)\n",
    "\n",
    "                    for item in merkmale:\n",
    "                        try:\n",
    "                            car_dict[item] = werte[merkmale.index(item)]\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    for key, value in dict(car_dict).items():\n",
    "                        if len(value) > 50 or len(key) > 50:\n",
    "                            del car_dict[key]\n",
    "\n",
    "\n",
    "                    multiple_cars_dict[URL] = car_dict\n",
    "                    counter += 1\n",
    "                    visited_urls.append(URL)\n",
    "                    if counter > 5:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(\"Detailseite: \" + str(e) + \" \" * 50)\n",
    "                    pass\n",
    "            print(\"\")\n",
    "        \n",
    "        # Action if no new offerings are available\n",
    "        else:\n",
    "            print(\"\\U0001F634\")\n",
    "            sleep(60)\n",
    "\n",
    "\n",
    "    if len(multiple_cars_dict) > 0:\n",
    "        df = pd.DataFrame(multiple_cars_dict).T\n",
    "\n",
    "        # delete less important columns\n",
    "        columns_to_remove = [\"Angebotsnummer\", \"Sicherheit\", \"Überführungskosten\", \"Zulassungskosten\",\n",
    "                            \"Gesamt, einmalig\", \"Leasinggesamtbetrag\", \"Vertragsart\", \"Fahrleistung p.a.\",\n",
    "                            \"Leasingfaktor\", \"Laufzeit\", \"Monatliche Rate\", \"Mehr-km Kosten\",\n",
    "                            \"Minder-km Vergütung\", \"Effektiver Jahreszins\", \"Sollzins geb. p.a.\",\n",
    "                            \"Teilkasko\", \"Vollkasko\", \"Schlussrate\", \"Gänge\",\n",
    "                            \"Sonderzahlung\", \"Folgeraten\", \"Erste Rate\",\n",
    "                            \"Nettodarlehensbetrag\", \"Nettodarlehen\", \"Bruttodarlehensbetrag\",\n",
    "                            \"Sollzinssatz\", \"Verfügbar ab\", \"Unnamed: 0\",\"Bearbeitungsgebühren\",\"Verfügbarkeit\",\n",
    "                             \"Letzter Zahnriemenwechsel\",\"Unnamed: 0.1\",\"Stromverbrauch2\",\"Anzahlung\"\n",
    "                            ]\n",
    "        for n in columns_to_remove:\n",
    "            try:\n",
    "                df = df.drop(n, 1)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Save as csv\n",
    "        df.to_csv(\".../Golf/\" + re.sub(\"[.,:,-, ]\", \"_\", str(datetime.now())) + \"BEFORE_WRANGLING.csv\", sep=\";\", index_label=\"url\")\n",
    "    else:\n",
    "        print(\"Keine Daten\")\n",
    "    with open(\".../data/visited/visited_urls.json\", \"w\") as file:\n",
    "        json.dump(visited_urls, file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d3968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
